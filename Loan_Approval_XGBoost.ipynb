{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class XGBClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, **params):\n",
        "        self.xgb = XGBClassifier(**params)\n",
        "\n",
        "    def fit(self, X, y, eval_set=None, **kwargs):\n",
        "        if eval_set:\n",
        "            self.xgb.fit(X, y, eval_set=eval_set, **kwargs)\n",
        "        else:\n",
        "            self.xgb.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.xgb.predict(X)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.xgb.predict_proba(X)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return self.xgb.score(X, y)\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/loan_data.csv')\n",
        "\n",
        "# Check the class distribution\n",
        "print(\"Original Class Distribution:\")\n",
        "print(data['loan_status'].value_counts(normalize=True))\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = data.drop('loan_status', axis=1)\n",
        "y = data['loan_status']\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_columns = [col for col in X.columns if X[col].dtype == 'object']\n",
        "\n",
        "# Create a preprocessor to handle categorical variables (without dropping the first category)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)],\n",
        "    remainder='passthrough')\n",
        "\n",
        "# Split into training and testing sets with stratification for balanced splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345, stratify=y)\n",
        "\n",
        "# Fit the preprocessor and transform the data\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# ========== Train XGBoost on the Original Data ==========\n",
        "xgb_model_orig = XGBClassifierWrapper(\n",
        "    random_state=12345,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    n_estimators=500,\n",
        "    early_stopping_rounds=10\n",
        ")\n",
        "\n",
        "# Cross-validation on the original dataset\n",
        "cv_scores_orig = cross_val_score(xgb_model_orig, X_train_transformed, y_train, cv=3, scoring='accuracy')\n",
        "print(f\"Cross-validation accuracy (Original Data, CV=3): {cv_scores_orig.mean():.4f}\")\n",
        "\n",
        "# Fit the model on the original dataset\n",
        "xgb_model_orig.fit(X_train_transformed, y_train, eval_set=[(X_test_transformed, y_test)], verbose=True)\n",
        "\n",
        "# Predict using the original dataset\n",
        "y_train_pred_orig = xgb_model_orig.predict(X_train_transformed)\n",
        "y_test_pred_orig = xgb_model_orig.predict(X_test_transformed)\n",
        "\n",
        "# ========== Apply RandomUnderSampler ==========\n",
        "rus = RandomUnderSampler(random_state=12345, sampling_strategy='auto')\n",
        "X_train_resampled, y_train_resampled = rus.fit_resample(X_train_transformed, y_train)\n",
        "\n",
        "# Check the new class distribution\n",
        "print(\"\\nNew Class Distribution after Undersampling:\")\n",
        "print(pd.Series(y_train_resampled).value_counts(normalize=True))\n",
        "\n",
        "# ========== Train XGBoost on the Undersampled Data ==========\n",
        "xgb_model_resampled = XGBClassifierWrapper(\n",
        "    random_state=12345,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    n_estimators=500,\n",
        "    early_stopping_rounds=10\n",
        ")\n",
        "\n",
        "# Cross-validation on the undersampled dataset\n",
        "cv_scores_resampled = cross_val_score(xgb_model_resampled, X_train_resampled, y_train_resampled, cv=3, scoring='accuracy')\n",
        "print(f\"Cross-validation accuracy (Resampled Data, CV=3): {cv_scores_resampled.mean():.4f}\")\n",
        "\n",
        "# Fit the model on the undersampled dataset\n",
        "xgb_model_resampled.fit(X_train_resampled, y_train_resampled, eval_set=[(X_test_transformed, y_test)], verbose=True)\n",
        "\n",
        "# Predict using the undersampled dataset\n",
        "y_train_pred_resampled = xgb_model_resampled.predict(X_train_resampled)\n",
        "y_test_pred_resampled = xgb_model_resampled.predict(X_test_transformed)\n",
        "\n",
        "# ========== Function to Compute Performance Metrics ==========\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, pos_label=1)\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
        "    conf_mat = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "    tn, fp, fn, tp = conf_mat.ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "    return accuracy, precision, recall, f1, specificity\n",
        "\n",
        "# Compute metrics for Original Data\n",
        "train_metrics_orig = calculate_metrics(y_train, y_train_pred_orig)\n",
        "test_metrics_orig = calculate_metrics(y_test, y_test_pred_orig)\n",
        "\n",
        "# Compute metrics for Undersampled Data\n",
        "train_metrics_resampled = calculate_metrics(y_train_resampled, y_train_pred_resampled)\n",
        "test_metrics_resampled = calculate_metrics(y_test, y_test_pred_resampled)\n",
        "\n",
        "# ========== Display Results ==========\n",
        "results_df = pd.DataFrame({\n",
        "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Specificity\"],\n",
        "    \"Train (Original)\": train_metrics_orig,\n",
        "    \"Test (Original)\": test_metrics_orig,\n",
        "    \"Train (Resampled)\": train_metrics_resampled,\n",
        "    \"Test (Resampled)\": test_metrics_resampled\n",
        "})\n",
        "\n",
        "print(\"\\n================== Performance Comparison ==================\\n\")\n",
        "print(results_df)\n",
        "\n",
        "# Display number of boosting rounds used\n",
        "print(f\"\\nNumber of boosting rounds used (Original Data): {xgb_model_orig.xgb.best_iteration}\")\n",
        "print(f\"Number of boosting rounds used (Resampled Data): {xgb_model_resampled.xgb.best_iteration}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D8A2DX7Sc3t",
        "outputId": "e68c5d16-2ead-4b9d-bf96-1047ae44be58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Class Distribution:\n",
            "loan_status\n",
            "0    0.777778\n",
            "1    0.222222\n",
            "Name: proportion, dtype: float64\n",
            "Cross-validation accuracy (Original Data, CV=3): 0.9329\n",
            "[0]\tvalidation_0-logloss:0.38579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [21:37:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\tvalidation_0-logloss:0.31831\n",
            "[2]\tvalidation_0-logloss:0.27645\n",
            "[3]\tvalidation_0-logloss:0.24853\n",
            "[4]\tvalidation_0-logloss:0.22859\n",
            "[5]\tvalidation_0-logloss:0.21442\n",
            "[6]\tvalidation_0-logloss:0.20378\n",
            "[7]\tvalidation_0-logloss:0.19601\n",
            "[8]\tvalidation_0-logloss:0.18909\n",
            "[9]\tvalidation_0-logloss:0.18468\n",
            "[10]\tvalidation_0-logloss:0.18028\n",
            "[11]\tvalidation_0-logloss:0.17766\n",
            "[12]\tvalidation_0-logloss:0.17541\n",
            "[13]\tvalidation_0-logloss:0.17350\n",
            "[14]\tvalidation_0-logloss:0.17215\n",
            "[15]\tvalidation_0-logloss:0.17088\n",
            "[16]\tvalidation_0-logloss:0.16920\n",
            "[17]\tvalidation_0-logloss:0.16860\n",
            "[18]\tvalidation_0-logloss:0.16706\n",
            "[19]\tvalidation_0-logloss:0.16655\n",
            "[20]\tvalidation_0-logloss:0.16583\n",
            "[21]\tvalidation_0-logloss:0.16536\n",
            "[22]\tvalidation_0-logloss:0.16421\n",
            "[23]\tvalidation_0-logloss:0.16385\n",
            "[24]\tvalidation_0-logloss:0.16297\n",
            "[25]\tvalidation_0-logloss:0.16251\n",
            "[26]\tvalidation_0-logloss:0.16247\n",
            "[27]\tvalidation_0-logloss:0.16118\n",
            "[28]\tvalidation_0-logloss:0.16098\n",
            "[29]\tvalidation_0-logloss:0.15995\n",
            "[30]\tvalidation_0-logloss:0.15978\n",
            "[31]\tvalidation_0-logloss:0.15936\n",
            "[32]\tvalidation_0-logloss:0.15912\n",
            "[33]\tvalidation_0-logloss:0.15899\n",
            "[34]\tvalidation_0-logloss:0.15890\n",
            "[35]\tvalidation_0-logloss:0.15786\n",
            "[36]\tvalidation_0-logloss:0.15738\n",
            "[37]\tvalidation_0-logloss:0.15719\n",
            "[38]\tvalidation_0-logloss:0.15660\n",
            "[39]\tvalidation_0-logloss:0.15653\n",
            "[40]\tvalidation_0-logloss:0.15625\n",
            "[41]\tvalidation_0-logloss:0.15630\n",
            "[42]\tvalidation_0-logloss:0.15586\n",
            "[43]\tvalidation_0-logloss:0.15590\n",
            "[44]\tvalidation_0-logloss:0.15570\n",
            "[45]\tvalidation_0-logloss:0.15592\n",
            "[46]\tvalidation_0-logloss:0.15535\n",
            "[47]\tvalidation_0-logloss:0.15522\n",
            "[48]\tvalidation_0-logloss:0.15498\n",
            "[49]\tvalidation_0-logloss:0.15488\n",
            "[50]\tvalidation_0-logloss:0.15448\n",
            "[51]\tvalidation_0-logloss:0.15446\n",
            "[52]\tvalidation_0-logloss:0.15456\n",
            "[53]\tvalidation_0-logloss:0.15419\n",
            "[54]\tvalidation_0-logloss:0.15433\n",
            "[55]\tvalidation_0-logloss:0.15381\n",
            "[56]\tvalidation_0-logloss:0.15392\n",
            "[57]\tvalidation_0-logloss:0.15410\n",
            "[58]\tvalidation_0-logloss:0.15396\n",
            "[59]\tvalidation_0-logloss:0.15448\n",
            "[60]\tvalidation_0-logloss:0.15464\n",
            "[61]\tvalidation_0-logloss:0.15438\n",
            "[62]\tvalidation_0-logloss:0.15440\n",
            "[63]\tvalidation_0-logloss:0.15463\n",
            "[64]\tvalidation_0-logloss:0.15449\n",
            "\n",
            "New Class Distribution after Undersampling:\n",
            "loan_status\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "Cross-validation accuracy (Resampled Data, CV=3): 0.8354\n",
            "[0]\tvalidation_0-logloss:0.52051\n",
            "[1]\tvalidation_0-logloss:0.42375\n",
            "[2]\tvalidation_0-logloss:0.36350\n",
            "[3]\tvalidation_0-logloss:0.32294\n",
            "[4]\tvalidation_0-logloss:0.29494\n",
            "[5]\tvalidation_0-logloss:0.27507\n",
            "[6]\tvalidation_0-logloss:0.26032\n",
            "[7]\tvalidation_0-logloss:0.25053\n",
            "[8]\tvalidation_0-logloss:0.24267\n",
            "[9]\tvalidation_0-logloss:0.23607\n",
            "[10]\tvalidation_0-logloss:0.23146\n",
            "[11]\tvalidation_0-logloss:0.22875\n",
            "[12]\tvalidation_0-logloss:0.22581\n",
            "[13]\tvalidation_0-logloss:0.22280\n",
            "[14]\tvalidation_0-logloss:0.22039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [21:37:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15]\tvalidation_0-logloss:0.21851\n",
            "[16]\tvalidation_0-logloss:0.21773\n",
            "[17]\tvalidation_0-logloss:0.21652\n",
            "[18]\tvalidation_0-logloss:0.21447\n",
            "[19]\tvalidation_0-logloss:0.21357\n",
            "[20]\tvalidation_0-logloss:0.21168\n",
            "[21]\tvalidation_0-logloss:0.21105\n",
            "[22]\tvalidation_0-logloss:0.21108\n",
            "[23]\tvalidation_0-logloss:0.21097\n",
            "[24]\tvalidation_0-logloss:0.21074\n",
            "[25]\tvalidation_0-logloss:0.21082\n",
            "[26]\tvalidation_0-logloss:0.21015\n",
            "[27]\tvalidation_0-logloss:0.20750\n",
            "[28]\tvalidation_0-logloss:0.20670\n",
            "[29]\tvalidation_0-logloss:0.20675\n",
            "[30]\tvalidation_0-logloss:0.20717\n",
            "[31]\tvalidation_0-logloss:0.20670\n",
            "[32]\tvalidation_0-logloss:0.20593\n",
            "[33]\tvalidation_0-logloss:0.20572\n",
            "[34]\tvalidation_0-logloss:0.20576\n",
            "[35]\tvalidation_0-logloss:0.20565\n",
            "[36]\tvalidation_0-logloss:0.20518\n",
            "[37]\tvalidation_0-logloss:0.20518\n",
            "[38]\tvalidation_0-logloss:0.20534\n",
            "[39]\tvalidation_0-logloss:0.20522\n",
            "[40]\tvalidation_0-logloss:0.20567\n",
            "[41]\tvalidation_0-logloss:0.20492\n",
            "[42]\tvalidation_0-logloss:0.20483\n",
            "[43]\tvalidation_0-logloss:0.20490\n",
            "[44]\tvalidation_0-logloss:0.20515\n",
            "[45]\tvalidation_0-logloss:0.20491\n",
            "[46]\tvalidation_0-logloss:0.20490\n",
            "[47]\tvalidation_0-logloss:0.20446\n",
            "[48]\tvalidation_0-logloss:0.20459\n",
            "[49]\tvalidation_0-logloss:0.20459\n",
            "[50]\tvalidation_0-logloss:0.20463\n",
            "[51]\tvalidation_0-logloss:0.20535\n",
            "[52]\tvalidation_0-logloss:0.20489\n",
            "[53]\tvalidation_0-logloss:0.20577\n",
            "[54]\tvalidation_0-logloss:0.20591\n",
            "[55]\tvalidation_0-logloss:0.20643\n",
            "[56]\tvalidation_0-logloss:0.20666\n",
            "\n",
            "================== Performance Comparison ==================\n",
            "\n",
            "        Metric  Train (Original)  Test (Original)  Train (Resampled)  \\\n",
            "0     Accuracy          0.952806         0.930778           0.943250   \n",
            "1    Precision          0.943046         0.894556           0.931072   \n",
            "2       Recall          0.838250         0.780500           0.957375   \n",
            "3     F1-Score          0.887565         0.833645           0.944040   \n",
            "4  Specificity          0.985536         0.973714           0.929125   \n",
            "\n",
            "   Test (Resampled)  \n",
            "0          0.896444  \n",
            "1          0.700903  \n",
            "2          0.931500  \n",
            "3          0.799914  \n",
            "4          0.886429  \n",
            "\n",
            "Number of boosting rounds used (Original Data): 55\n",
            "Number of boosting rounds used (Resampled Data): 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrices for Original Data\n",
        "conf_mat_orig_train = confusion_matrix(y_train, y_train_pred_orig)\n",
        "conf_mat_orig_test = confusion_matrix(y_test, y_test_pred_orig)\n",
        "\n",
        "# Compute confusion matrices for Undersampled Data\n",
        "conf_mat_resampled_train = confusion_matrix(y_train_resampled, y_train_pred_resampled)\n",
        "conf_mat_resampled_test = confusion_matrix(y_test, y_test_pred_resampled)\n",
        "\n",
        "# Print Confusion Matrices\n",
        "print(\"\\nConfusion Matrix for Original Data (Train):\")\n",
        "print(conf_mat_orig_train)\n",
        "\n",
        "print(\"\\nConfusion Matrix for Original Data (Test):\")\n",
        "print(conf_mat_orig_test)\n",
        "\n",
        "print(\"\\nConfusion Matrix for Resampled Data (Train):\")\n",
        "print(conf_mat_resampled_train)\n",
        "\n",
        "print(\"\\nConfusion Matrix for Resampled Data (Test):\")\n",
        "print(conf_mat_resampled_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFlw6zqNXIzQ",
        "outputId": "f5427486-1d3c-443d-972c-86f5c66afdac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix for Original Data (Train):\n",
            "[[27595   405]\n",
            " [ 1294  6706]]\n",
            "\n",
            "Confusion Matrix for Original Data (Test):\n",
            "[[6816  184]\n",
            " [ 439 1561]]\n",
            "\n",
            "Confusion Matrix for Resampled Data (Train):\n",
            "[[7433  567]\n",
            " [ 341 7659]]\n",
            "\n",
            "Confusion Matrix for Resampled Data (Test):\n",
            "[[6205  795]\n",
            " [ 137 1863]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import RFECV\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/loan_data.csv')\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = data.drop('loan_status', axis=1)\n",
        "y = data['loan_status']\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_columns = [col for col in X.columns if X[col].dtype == 'object']\n",
        "\n",
        "# Create a preprocessor to handle categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)],\n",
        "    remainder='passthrough')\n",
        "\n",
        "# Split into training and testing sets with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345, stratify=y)\n",
        "\n",
        "# Fit the preprocessor and transform the data\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Apply RandomUnderSampler for undersampling\n",
        "rus = RandomUnderSampler(random_state=12345, sampling_strategy='auto')\n",
        "X_train_resampled, y_train_resampled = rus.fit_resample(X_train_transformed, y_train)\n",
        "\n",
        "# Print class distribution after undersampling\n",
        "print(\"Class distribution after undersampling:\")\n",
        "print(pd.Series(y_train_resampled).value_counts())\n",
        "\n",
        "# Define a function to train and evaluate the model\n",
        "def train_and_evaluate(X_train_data, y_train_data, X_test_data, y_test_data):\n",
        "    # Initialize the base XGBClassifier\n",
        "    xgb_base = XGBClassifier(random_state=12345, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "    # Use RFECV for feature selection\n",
        "    rfecv = RFECV(estimator=xgb_base, step=1, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "    X_train_selected = rfecv.fit_transform(X_train_data, y_train_data)\n",
        "    X_test_selected = rfecv.transform(X_test_data)\n",
        "\n",
        "    # Get feature names after one-hot encoding\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "    # Filter selected features using RFECV support mask\n",
        "    selected_features = np.array(feature_names)[rfecv.support_]\n",
        "\n",
        "    print(\"\\nSelected Features:\")\n",
        "    print(selected_features)\n",
        "\n",
        "    # Print the number of selected features\n",
        "    print(f\"\\nOptimal number of features: {rfecv.n_features_}\")\n",
        "\n",
        "    # Define the parameter grid for XGBoost\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 500],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'colsample_bytree': [0.8, 1.0]\n",
        "    }\n",
        "\n",
        "    # Initialize GridSearchCV with feature-selected data\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=XGBClassifier(random_state=12345, use_label_encoder=False, eval_metric='logloss'),\n",
        "        param_grid=param_grid,\n",
        "        cv=3,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train_selected, y_train_data)\n",
        "\n",
        "    # Print the best parameters\n",
        "    print(\"Best parameters from GridSearchCV:\", grid_search.best_params_)\n",
        "\n",
        "    # Train the final model with the best parameters\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=grid_search.best_params_['n_estimators'],\n",
        "        max_depth=grid_search.best_params_['max_depth'],\n",
        "        learning_rate=grid_search.best_params_['learning_rate'],\n",
        "        subsample=grid_search.best_params_['subsample'],\n",
        "        colsample_bytree=grid_search.best_params_['colsample_bytree'],\n",
        "        random_state=12345,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "\n",
        "    xgb_model.fit(X_train_selected, y_train_data)\n",
        "\n",
        "    # Predict on training and test sets\n",
        "    y_train_pred = xgb_model.predict(X_train_selected)\n",
        "    y_test_pred = xgb_model.predict(X_test_selected)\n",
        "\n",
        "    # Compute and print training and test metrics\n",
        "    def compute_metrics(y_true, y_pred):\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "        recall = tp / (tp + fn)  # Sensitivity (True Positive Rate)\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        specificity = tn / (tn + fp)  # True Negative Rate\n",
        "\n",
        "        return accuracy, recall, precision, specificity\n",
        "\n",
        "    # Compute and print training metrics\n",
        "    train_accuracy, train_recall, train_precision, train_specificity = compute_metrics(y_train_data, y_train_pred)\n",
        "    print(\"\\nTraining Set Metrics:\")\n",
        "    print(f\"Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Recall (Sensitivity): {train_recall:.4f}\")\n",
        "    print(f\"Precision: {train_precision:.4f}\")\n",
        "    print(f\"Specificity: {train_specificity:.4f}\")\n",
        "\n",
        "    # Compute and print test metrics\n",
        "    test_accuracy, test_recall, test_precision, test_specificity = compute_metrics(y_test_data, y_test_pred)\n",
        "    print(\"\\nTest Set Metrics:\")\n",
        "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Recall (Sensitivity): {test_recall:.4f}\")\n",
        "    print(f\"Precision: {test_precision:.4f}\")\n",
        "    print(f\"Specificity: {test_specificity:.4f}\")\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report (Training Set):\")\n",
        "    print(classification_report(y_train_data, y_train_pred))\n",
        "\n",
        "    print(\"\\nClassification Report (Test Set):\")\n",
        "    print(classification_report(y_test_data, y_test_pred))\n",
        "\n",
        "    # Print confusion matrices for both training and test sets\n",
        "    print(\"\\nConfusion Matrix (Training Set):\")\n",
        "    print(confusion_matrix(y_train_data, y_train_pred))\n",
        "\n",
        "    print(\"\\nConfusion Matrix (Test Set):\")\n",
        "    print(confusion_matrix(y_test_data, y_test_pred))\n",
        "\n",
        "    return xgb_model\n",
        "\n",
        "# Train and evaluate on the original (imbalanced) data\n",
        "print(\"\\n--- Results for Original (Imbalanced) Class Distribution ---\")\n",
        "xgb_model_imbalanced = train_and_evaluate(X_train_transformed, y_train, X_test_transformed, y_test)\n",
        "\n",
        "# Train and evaluate on the undersampled data\n",
        "print(\"\\n--- Results for Undersampled Class Distribution ---\")\n",
        "xgb_model_undersampled = train_and_evaluate(X_train_resampled, y_train_resampled, X_test_transformed, y_test)\n",
        "\n",
        "# Save both models\n",
        "joblib.dump(xgb_model_imbalanced, '/content/xgboost_model_imbalanced.pkl')\n",
        "joblib.dump(xgb_model_undersampled, '/content/xgboost_model_undersampled.pkl')"
      ],
      "metadata": {
        "id": "R8tnzb_aY_Q2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}